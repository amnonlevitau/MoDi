from evaluation.models.stgcn import STGCN
import argparse
import re
import pandas as pd
import os.path as osp
import os
import datetime

import torch

from torch.utils.data import DataLoader
from model import Generator, Discriminator
import numpy as np
from utils.data import anim_from_edge_rot_dict
from utils.visualization import motion2fig, motion2bvh, one_motion2bvh, edge_rot_dict_from_edge_motion_data
from traits import NonSkeletonAwareTraits, SkeletonAwareConv2DTraits, SkeletonAwareConv3DTraits, SkeletonAwarePoolTraits
import matplotlib.pyplot as plt
import sys as _sys
from utils.data import Joint, Edge # used by the 'eval' command
from evaluation.action2motion.fid import calculate_fid
from evaluation.action2motion.diversity import calculate_diversity
from evaluation.metrics.kid import calculate_kid
from evaluation.metrics.precision_recall import precision_and_recall
from tqdm import tqdm
from Motion import Animation, BVH
from matplotlib import pyplot as plt
from utils.data import motion_from_raw
from generate import get_gen_mot_np
from generate import sample



# datapath = 'evaluation/data/humanact12motion.npy'
DEBUG = False

def generate(args, g_ema, discriminator, device, mean_joints, std_joints, num_motions, entity):
    args.sample_seeds = None
    args.no_idle = False
    with torch.no_grad():
        g_ema.eval()
        mean_latent = g_ema.mean_latent(4096)
        args.motions = num_motions
        args.truncation = 1
        args.n_clusters = 1
        args.rotation_repr = 'quat'
        generated_motions = sample(args, g_ema, device, mean_latent, mean_joints, std_joints)

    generated_motion = generated_motions.motion
    # convert motion to numpy
    if isinstance(generated_motion, pd.Series): # yes
        index = generated_motion.index
        if not isinstance(generated_motion.iloc[0], list) and \
                generated_motion.iloc[0].ndim == 4 and generated_motion.iloc[0].shape[0] > 1:
            generated_motion = generated_motion.apply(lambda motions: torch.unsqueeze(motions, 1)) # add a batch dimension
            generated_motion = generated_motion.apply(list) # get_gen_mot_np expects lists
        generated_motion = generated_motion.tolist()
    else:
        assert isinstance(generated_motion, list)
        index = range(len(generated_motion))

    generated_motion_np, _ = get_gen_mot_np(args, generated_motion, mean_joints, std_joints)
    # hack. todo: change to keeping some of the info (e.g., parents) and predicting or keeping other info (e.g., offsets)
    generated_motions = np.concatenate(generated_motion_np, axis=0)

    if entity.str() == 'Joint':
        edge_rot_dict_general = None
        return generated_motions

    # else entity = Edge:
    if args.dataset == 'mixamo':
        gt_path = '/home/sigalr/data/mixamo/bvh/all_bin_30_fps/edge_rot_joints_1.npy'  # todo: fix this temporary hack
    elif args.dataset == 'humanact12':
        gt_path = '/home/inball1/datasets/HumanAct12_bvh_ordered/edge_rot_joints_1.npy'
    # gt_path = '/home/inball1/datasets/HumanAct12_bvh_ordered/edge_rot_joints_1.npy'
    _, _, _, edge_rot_dict_general = motion_from_raw(args, np.load(gt_path, allow_pickle=True))
    edge_rot_dict_general['std_tensor'] = edge_rot_dict_general['std_tensor'].cpu()
    edge_rot_dict_general['mean_tensor'] = edge_rot_dict_general['mean_tensor'].cpu()
    if args.dataset == 'mixamo':
        edge_rot_dict_general['offsets_no_root'] /= 100 ## not needed in humanact
    # edge_rot_dict_general = np.load(gt_path, allow_pickle=True)[0]

    generated_motions = []

    for i, idx in enumerate(index):
        id = idx if idx is not None else i
        # get anim for xyz positions
        motion_data = generated_motion_np[i] * edge_rot_dict_general['std'].transpose(0, 2, 1, 3) + edge_rot_dict_general['mean'].transpose(0, 2, 1, 3) #rescale edge motion to the size of joint location motions
        from utils.data import to_list_4D
        motion_data = to_list_4D(motion_data)
        anim_dicts, frame_mults, is_sub_motion = edge_rot_dict_from_edge_motion_data(motion_data, type='sample', edge_rot_dict_general = edge_rot_dict_general)
        for j, (anim_dict, frame_mult) in enumerate(zip(anim_dicts, frame_mults)):
            anim, names = anim_from_edge_rot_dict(anim_dict, root_name='Hips')  # todo: fix the hard coding
            # compute global positions using anim
            positions = Animation.positions_global(anim)

            # sample joints relevant to 15 joints skeleton
            positions_15joints = positions[:, [7, 6, 15, 16, 17, 10, 11, 12, 0, 23, 24, 25, 19, 20, 21]] # openpose order R then L
            positions_15joints = positions_15joints.transpose(1, 2, 0)
            positions_15joints_oriented = positions_15joints.copy()
            if args.dataset=='mixamo':
                positions_15joints_oriented = positions_15joints_oriented[:, [0, 2, 1]]
                positions_15joints_oriented[:, 1, :] = -1 * positions_15joints_oriented[:, 1, :]
            generated_motions.append(positions_15joints_oriented)

            if DEBUG:
                # BVH using edge rotations
                motion2bvh(generated_motion_np[i], osp.join('evaluation/data', 'generated_edges.bvh'.format(id)),
                           new_ik=True, parents=entity.parents_list, type=args.type, entity=entity.str(),
                           edge_rot_dict_general=edge_rot_dict_general)
                # sanity check - remove duplicate joints and convert to bvh using joint positions
                positions_sampled = positions[:,[0, 2, 3, 4, 6, 7, 9, 10, 11, 12, 14, 15, 16, 17, 19, 20, 21, 23, 24, 25]]
                parents = [-1, 0, 1, 2, 3, 4, 3, 6, 7, 8, 3, 10, 11, 12, 0, 14, 15, 0, 17, 18]
                positions_sampled = positions_sampled.transpose(1,2,0)
                one_motion2bvh(positions_sampled, osp.join('evaluation/data', 'generated_sampled.bvh'), parents=parents, new_ik=True, is_openpose=False)
                # visualize 15 joint skeleton
                one_motion2bvh(positions_15joints, osp.join('evaluation/data', 'generated_sampled_15joints.bvh'), parents=parents, new_ik=True, is_openpose=True)
                # align orientation to match joint location
                one_motion2bvh(positions_15joints_oriented, osp.join('evaluation/data', 'generated_sampled_15joints_oriented.bvh'), parents=parents, new_ik=True, is_openpose=True)

    generated_motions = np.asarray(generated_motions)
    return generated_motions

def _parse_num_range(s):
    '''Accept either a comma separated list of numbers 'a,b,c' or a range 'a-c' and return as a list of ints.'''

    range_re = re.compile(r'^(\d+)-(\d+)$')
    m = range_re.match(s)
    if m:
        return list(range(int(m.group(1)), int(m.group(2))+1))
    vals = s.split(',')
    return [int(x) for x in vals]

def _parse_list_num_ranges(s):
    ''' accept comma seperated list of ranges 'a-c','d-e' and return list of lists of int [[a,b,c],[d,e]]'''
    ranges = s.split(',')
    return [_parse_num_range(r) for r in ranges]
#endregion

def calculate_activation_statistics(activations):
    activations = activations.cpu().detach().numpy()
    # activations = activations.cpu().numpy()
    mu = np.mean(activations, axis=0)
    sigma = np.cov(activations, rowvar=False)
    return mu, sigma

def initialize_model(device, modelpath, dataset='mixamo'):
    if dataset == 'mixamo':
        num_classes = 15
    elif dataset == 'humanact12':
        num_classes = 12
    model = STGCN(in_channels=3,
                  num_class=num_classes,
                  graph_args={"layout": 'openpose', "strategy": "spatial"},
                  edge_importance_weighting=True,
                  device=device)
    model = model.to(device)
    state_dict = torch.load(modelpath, map_location=device)
    model.load_state_dict(state_dict)
    model.eval()
    return model

def compute_features(model, iterator):
    device = 'cuda'
    activations = []
    predictions = []
    with torch.no_grad():
        for i, batch in enumerate(iterator):
        # for i, batch in tqdm(enumerate(iterator), desc="Computing batch"):
            batch_for_model = {}
            batch_for_model['x'] = batch.to(device).float()
            model(batch_for_model)
            activations.append(batch_for_model['features'])
            predictions.append(batch_for_model['yhat'])
            # labels.append(batch_for_model['y'])
        activations = torch.cat(activations, dim=0)
        predictions = torch.cat(predictions, dim=0)
        # labels = torch.cat(labels, dim=0)
        # shape torch.Size([16, 15, 3, 64]) (batch, joints, xyz, frames)
    return activations, predictions


def main(args_not_parsed):

    #region configurations
    device = "cuda"
    parser = argparse.ArgumentParser(description="Generate samples from the generator and compute action recognition model features")
    parser.add_argument(
        "--motions", type=int, default=20, help="number of motions to be generated"
    )
    parser.add_argument("--type", type=str, default='sample', choices=['sample','truncation_series','interp', 'mix', 'nearest', 'edit', 'interp-mix-pyramid'],
                        help="generation type: \n"
                             "regular: generate one motion batch\n"
                             "truncation_series: generate a series of motions that differ by truncation of W only) \n"
                             "interpolate: interpolate W space of two random motions \n"
                             "mix: mix two images \n"
                             "edit: latent space editing")
    # related to sample
    parser.add_argument('--sample_seeds', type=_parse_num_range, help='Seeds to use for generation')
    parser.add_argument('--return_sub_motions', action='store_true', help='Return motions created by coarse pyramid levels')
    parser.add_argument('--no_idle', action='store_true', help='sample only non-idle motions')

    parser.add_argument("--truncation", type=float, default=1, help="truncation ratio")
    parser.add_argument(
        "--truncation_mean",
        type=int,
        default=4096,
        help="number of vectors to calculate mean for the truncation",
    )
    parser.add_argument(
        "--ckpt",
        type=str,
        # default = '/home/inball1/train_outputs/stylegan2_motion_skeleton/train_humanact12_skeleton_conv3_no_mixing.a146b827c3be41df848d41b63229df74/models/080000.pt', # no mixing conv3
        # default = '/home/inball1/train_outputs/stylegan2_motion_skeleton/humanact12_skeleton_joints_pool_no_mixing.ace5443c12a0467bbe158bd19559b2f9/models/080000.pt', # no mixing joints pool
        # default = '/home/inball1/train_outputs/stylegan2_motion_skeleton/train_humanact12_edge_rotation_mixing_0p9_conv3.24032cd88c7c47649a57865efba0535d/models/050000.pt', ## edge humanact12
        # default = '/home/inball1/train_outputs/stylegan2_motion_skeleton/humanact12_skeleton_conv3_edge_mixing_0p9_fix_edge_order.321f2dcfbcdc4517aef74c396727de55/models/014000.pt', ## edge humanact12 fix order
        # default='/home/inball1/train_outputs/stylegan2_motion_skeleton/parents_bug/Jasper_all_5K_no_norm_mixing_0p9_conv3_jointloc_recstyle1_recmotion1.982_8de_079999.pt',         ## joint mixamo bug parents
        default ='/home/inball1/train_outputs/stylegan2_motion_skeleton/mixamo_train_joints_pool_demod_data_fix_pooling_dist_1.7a545552d62947a798f643724cf685c2/models/080000.pt',
        # default ='/home/inball1/train_outputs/stylegan2_motion_skeleton/mixamo_train_joints_pool_demod_data_fix_pooling_dist_1.7a545552d62947a798f643724cf685c2/models/010000.pt',
        help="path to the model checkpoint",
    )
    parser.add_argument(
        "--dataset",
        type=str,
        default='mixamo',
        help='mixamo or hmmanact12'
    )
    parser.add_argument(
        "--channel_multiplier",
        type=int,
        default=2,
        help="channel multiplier of the generator. config-f = 2, else = 1",
    )
    parser.add_argument(
        "--rot_only", action="store_true",
        help="refrain from predicting global root position when predicting rotations"
    )
    parser.add_argument(
        "--test_model", action="store_true",
        help="generate motions with model and evaluate"
    )
    parser.add_argument(
        "--test_actor", action="store_true",
        help="evaluate results from ACTOR model"
    )

    parser.add_argument('--path', type=str, help='Path to ground truth file that was used during train. Not needed unless one wants to override the local path saved by the network')
    parser.add_argument('--fast', action='store_true', help='Skip metrics that require long evaluation')
    parser.add_argument('--out_path', type=str, help='Path to output folder. If not provided, output folder will be <ckpt/ckpt_files/timestamp')

    args = parser.parse_args(args=args_not_parsed)

    args.latent = 512
    args.n_mlp = 8

    checkpoint = torch.load(args.ckpt)
    network_args_unique = ['skeleton', 'entity', 'glob_pos', 'joints_pool', 'conv3', 'foot',
                           'reconstruct_latent', 'reconstruct_motion', 'latent_rec_idx',
                           'g_latent_reg_weight', 'd_latent_reg_weight', 'normalize',
                           'axis_up', 'use_velocity']
    network_args_non_unique = ['path']
    loaded_args = checkpoint['args']
    for arg_name in network_args_unique + network_args_non_unique:
        assert not hasattr(args, arg_name) or arg_name in network_args_non_unique  # make sure network args don't coincide with command line args
        if arg_name in network_args_unique or getattr(args, arg_name, None) is None:
            arg_val = getattr(loaded_args, arg_name, None)
            setattr(args, arg_name, arg_val)

    if not (getattr(args, 'test_model', None) ^ getattr(args, 'test_actor', None)):
        setattr(args, 'test_model', True)
        setattr(args, 'test_actor', False)


    if args.skeleton:
        if args.joints_pool:
            traits_class = SkeletonAwarePoolTraits
        elif args.conv3:
            traits_class = SkeletonAwareConv3DTraits
        else:
            traits_class = SkeletonAwareConv2DTraits
    else:
        traits_class = NonSkeletonAwareTraits

    if args.glob_pos:
        # add global position adjacency nodes to neighbouring lists
        # this method must be called BEFORE the Generator and the Discriminator are initialized
        Edge.enable_global_position()

    if args.foot:
        Edge.enable_foot_contact()

    entity = eval(args.entity)
    #endregion

    #region generator
    g_ema = Generator(
        args.latent, args.n_mlp, traits_class = traits_class, entity=entity
    ).to(device)

    g_ema.load_state_dict(checkpoint["g_ema"])

    if args.latent_rec_idx is None: args.latent_rec_idx = traits_class.n_levels(entity)
    if args.reconstruct_latent is None:
        if args.g_latent_reg_weight and args.g_latent_reg_weight > 0 or args.d_latent_reg_weight and args.d_latent_reg_weight > 0: args.reconstruct_latent = True
        else: args.reconstruct_latent = False
    if args.reconstruct_motion is None: args.reconstruct_motion = False

    discriminator = Discriminator(channel_multiplier=args.channel_multiplier, traits_class=traits_class,
                                  entity=entity, reconstruct_latent=args.reconstruct_latent or args.reconstruct_motion,
                                  latent_dim=args.latent, n_latent=g_ema.n_latent,
                                  latent_rec_idx=int(args.latent_rec_idx)
                                  ).to(device)

    discriminator.load_state_dict(checkpoint["d"])
    mean_joints = checkpoint['mean_joints']
    std_joints = checkpoint['std_joints']

    #endregion
    modelpath = "evaluation/checkpoint_0300_mixamo_acc_0.74_train_test_split_smaller_arch.tar"
    if args.dataset == 'humanact12':
        modelpath = 'evaluation/humanact12_checkpoint_0150_acc_1.0.pth.tar'

    # initialize model
    model = initialize_model(device, modelpath, args.dataset)

    if args.test_model:
        # generate motions
        generated_motions = generate(args, g_ema, discriminator, device, mean_joints, std_joints,num_motions=args.motions, entity=entity)
        generated_motions = generated_motions[:, :15]

    elif args.test_actor:
        # generate motions
        # actor_res = np.load('/home/inball1/Actor results/ACTOR_res_humanact12.npy', allow_pickle=True)
        actor_res = np.load('/home/inball1/Actor results/ACTOR_res_humanact12_glob.npy', allow_pickle=True)
        generated_motions = actor_res[:, :15]

    generated_motions -= generated_motions[:, 8:9, :, :]  # locate root joint of all frames at origin

    iterator_generated = DataLoader(generated_motions, batch_size=64, shuffle=False, num_workers=8)

    # compute features of generated motions
    generated_features, generated_predictions = compute_features(model, iterator_generated)
    generated_stats = calculate_activation_statistics(generated_features)


    # dataset motions
    motion_data_raw = np.load(args.path, allow_pickle=True)
    motion_data = motion_data_raw[:, :15]
    # motion_data -= np.expand_dims(motion_data[:, 8, :, 0], (1, 3))  # locate root joint of initial pose at origin
    motion_data -= motion_data[:, 8:9, :, :]  # locate root joint of all frames at origin
    iterator_dataset = DataLoader(motion_data, batch_size=64, shuffle=False, num_workers=8)

    # compute features of dataset motions
    dataset_features, dataset_predictions = compute_features(model, iterator_dataset)
    real_stats = calculate_activation_statistics(dataset_features)

    # mirrored dataset motions
    # motion_data[:, :, 2, :] = motion_data[:, :, 2, :] * -1 # mirror good - 0 or 2 (1 upside down)
    # iterator_mirrored_dataset = DataLoader(motion_data, batch_size=64, shuffle=False, num_workers=8)
    # compute features of mirrored dataset motions
    # mirrored_features, mirrored_predictions = compute_features(model, iterator_mirrored_dataset)
    # mirror_stats = calculate_activation_statistics(mirrored_features)

    print(f"evaluation resutls for model {args.ckpt}\n")

    fid = calculate_fid(generated_stats, real_stats)
    print(f"FID score: {fid}\n")
    # fid_m = calculate_fid(mirror_stats, real_stats)
    # print("FID mirror score: ", fid_m)

    print("calculating KID...")
    kid = calculate_kid(dataset_features.cpu(), generated_features.cpu())
    (m, s) = kid
    print('KID : %.3f (%.3f)' % (m, s))
    print()

    dataset_diversity = calculate_diversity(dataset_features)
    generated_diversity = calculate_diversity(generated_features)
    # mirrored_diversity = calculate_diversity(mirrored_features)
    print(f"Diversity of generated motions: {generated_diversity}")
    print(f"Diversity of dataset motions: {dataset_diversity}\n")
    # print(f"Diversity mirrored dataset: {mirrored_diversity}")

    if args.fast:
        print("Skipping MMD calculation\n")
        mmd_score_mean = mmd_score_std = None
    else:
        print("calculating MMD score")
        from evaluation.metrics.mmd import mmd_function
        mmd_scores = []
        for i in tqdm(range(10000)):
            idx1 = np.random.randint(0,generated_motions.shape[0])
            idx2 = np.random.randint(0,motion_data.shape[0])
            gen = generated_motions[idx1]
            real = motion_data[idx2]
            mmd_score = mmd_function(gen, real, 'MMDS')
            mmd_scores.append(mmd_score)
        mmd_scores = np.asarray(mmd_scores)
        mmd_score_mean = mmd_scores.mean()
        mmd_score_std = np.std(mmd_scores)
        print("MMD score: ", mmd_score_mean, "+-", mmd_score_std, "\n")

    if args.fast:
        print("Skipping precision-recall calculation\n")
        precision = recall = None
    else:
        print("calculating precision recall...")
        precision, recall = precision_and_recall(generated_features, dataset_features)
        print(f"precision: {precision}")
        print(f"recall: {recall}\n")

    #plot histogram of predictions
    yhat = generated_predictions.max(dim=1).indices
    fig_hist_generated = plt.figure()
    plt.bar(*np.unique(yhat.cpu(), return_counts=True))
    plt.title(f'generated {args.ckpt}')
    plt.show()

    yhat = dataset_predictions.max(dim=1).indices
    fig_hist_dataset = plt.figure()
    plt.bar(*np.unique(yhat.cpu(), return_counts=True))
    plt.title('dataset')
    plt.show()

    # yhat = mirrored_predictions.max(dim=1).indices
    # plt.bar(*np.unique(yhat.cpu(), return_counts=True))
    # plt.title('dataset mirrored ')
    # plt.show()

    save_results(args, fid, kid, (generated_diversity, dataset_diversity) ,(mmd_score_mean, mmd_score_std),
                 (precision, recall), fig_hist_generated, fig_hist_dataset)


def save_results(args, fid, kid, diversity ,mmd, prec_rec, fig_hist_g, fig_hist_r):

    # define output path
    if args.out_path is not None:
        out_path = args.out_path
        os.makedirs(out_path, exist_ok=True)
    else:
        time_str = datetime.datetime.now().strftime('%y_%m_%d_%H_%M')
        out_path = osp.join(osp.splitext(args.ckpt)[0] + '_files', f'{time_str}_eval')
        os.makedirs(out_path, exist_ok=True)

    # same numeric results
    num_res_path = osp.join(out_path, 'eval.csv')
    pd.Series({'fid': fid, 'kid':kid, 'diversity':diversity, 'prec_rec':prec_rec, 'mmd':mmd}).to_csv(num_res_path, sep='\t', header=None) # save args
    args_path = osp.join(out_path, 'args.csv')
    pd.Series(args.__dict__).to_csv(args_path, sep='\t', header=None) # save args

    # save distribution images
    fig_name = osp.join(out_path, 'distribution_g.png')
    fig_hist_g.savefig(fig_name)
    fig_name = osp.join(out_path, 'distribution_r.png')
    fig_hist_r.savefig(fig_name)


if __name__ == '__main__':
    main(_sys.argv[1:])
